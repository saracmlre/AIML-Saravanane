{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyML9bgpMmGOJdrv1oll84VK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Attention and Transformers"],"metadata":{"id":"VWd-sHzcII-w"}},{"cell_type":"markdown","source":["###**Why is it important to learn about Transformers?**\n","\n","Since they were first introduced in Attention Is All You Need (2017), Transformers have been the state-of-the-art for natural language processing. Recently, we have also seen Transformers applied to computer vision tasks with very promising results (see DETR, ViT). Vision Transformers, for example, now outperform all CNN-based models for image classification! Many people in the deep learning community (myself included) believe that an attention revolution is imminent — that is, that attention-based models will soon replace most of the existing state-of-the-art methods.\n","\n","All deep learning practitioners should familiarize themselves with Transformers in the near future. Plenty of other Transformer articles exist, both on Medium and across the web. But I learn best by doing, so I set out to build my own PyTorch implementation. In this article, I hope to bring a new perspective and encourage others to join the revolution."],"metadata":{"id":"wKBCR3gOIMDG"}},{"cell_type":"markdown","source":["### **Attention Mechanisms**\n","\n","As the title “Attention Is All You Need” suggests, Transformers are centered around attention mechanisms. Attention is described in the paper’s abstract:\n","\n","```\n","\"Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences.\"\n","```\n","\n","In plain English, attention mechanisms relate data points within sequences. And they are very good at doing that. Transformers use a specific type of attention mechanism, referred to as multi-head attention. This is the most important part of the model! We will be learning about both multi-headed attention and Scaled Dot-Product Attention.\n"],"metadata":{"id":"o5m9hB5RIPDp"}},{"cell_type":"markdown","source":["<img src=\"https://miro.medium.com/max/1400/1*BzhKcJJxv974OxWOVqUuQQ.png\">"],"metadata":{"id":"WG0AzoMmISH5"}},{"cell_type":"markdown","source":["### **Scaled Dot-Product Attention**\n","\n","Let us start with the mathematical expression:\n","<img src=\"https://miro.medium.com/max/882/1*4pWBd6sgTnr0YieOyRSZVQ.png\">\n","\n","What exactly is happening here? Q, K, and V are batches of matrices, each with shape **(batch_size, seq_length, num_features)**. Multiplying the **query (Q)** and **key (K)** arrays results in a (batch_size, seq_length, seq_length) array, which tells us roughly how important each element in the sequence is. This is the attention of this layer — it determines which elements we “pay attention” to. The attention array is normalized using softmax, so that all of the weights sum to one. (Because we can’t pay more than 100% attention, right?) Finally, the attention is applied to the **value (V)** array using matrix multiplication."],"metadata":{"id":"BZVryPsxIW4K"}},{"cell_type":"code","source":["from torch import Tensor\n","import torch.nn.functional as f\n","\n","\n","def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n","    temp = query.bmm(key.transpose(1, 2))\n","    scale = query.size(-1) ** 0.5\n","    softmax = f.softmax(temp / scale, dim=-1)\n","    return softmax.bmm(value)"],"metadata":{"id":"PktLsmwkIKIr","executionInfo":{"status":"ok","timestamp":1695474111396,"user_tz":-330,"elapsed":5630,"user":{"displayName":"Nayan JHA","userId":"08243209700018571801"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["From the diagram above, we see that multi-head attention is composed of several identical attention heads. Each attention head contains 3 linear layers, followed by scaled dot-product attention. Let’s encapsulate this in an AttentionHead layer:"],"metadata":{"id":"IxyGYWNeIeLj"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","\n","\n","class AttentionHead(nn.Module):\n","    def __init__(self, dim_in: int, dim_q: int, dim_k: int):\n","        super().__init__()\n","        self.q = nn.Linear(dim_in, dim_q)\n","        self.k = nn.Linear(dim_in, dim_k)\n","        self.v = nn.Linear(dim_in, dim_k)\n","\n","    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n","        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))"],"metadata":{"id":"YZhZF2quIZwY","executionInfo":{"status":"ok","timestamp":1695474131639,"user_tz":-330,"elapsed":17,"user":{"displayName":"Nayan JHA","userId":"08243209700018571801"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Now, it’s very easy to build the multi-head attention layer. Just combine num_heads different attention heads and a Linear layer for the output."],"metadata":{"id":"WLKlWpwpImFI"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, num_heads: int, dim_in: int, dim_q: int, dim_k: int):\n","        super().__init__()\n","        self.heads = nn.ModuleList(\n","            [AttentionHead(dim_in, dim_q, dim_k) for _ in range(num_heads)]\n","        )\n","        self.linear = nn.Linear(num_heads * dim_k, dim_in)\n","\n","    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n","        return self.linear(\n","            torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n","        )"],"metadata":{"id":"7dbJN1JHIh3I","executionInfo":{"status":"ok","timestamp":1695474161278,"user_tz":-330,"elapsed":11,"user":{"displayName":"Nayan JHA","userId":"08243209700018571801"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Let’s pause again to examine what’s going on in the MultiHeadAttention layer. Each attention head computes its own query, key, and value arrays, and then applies scaled dot-product attention. Conceptually, this means each head can attend to a different part of the input sequence, independent of the others. Increasing the number of attention heads allows us to “pay attention” to more parts of the sequence at once, which makes the model more powerful."],"metadata":{"id":"S6yT-2qMIsOY"}},{"cell_type":"markdown","source":["### **Positional Encoding**\n","We need one more component before building the complete transformer: positional encoding. Notice that MultiHeadAttention has no trainable components that operate over the sequence dimension (axis 1). Everything operates over the feature dimension (axis 2), and so it is independent of sequence length. We have to provide positional information to the model, so that it knows about the relative position of data points in the input sequences.\n","\n","Vaswani et. al. encode positional information using trigonometric functions, according to the equation:\n","\n","<img src=\"https://miro.medium.com/max/912/1*C3a9RL6-SFC6fW8NGpJg5A.png\">"],"metadata":{"id":"hqir1D3MIvMB"}},{"cell_type":"code","source":["def position_encoding(\n","    seq_len: int, dim_model: int, device: torch.device = torch.device(\"cpu\"),\n",") -> Tensor:\n","    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n","    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n","    phase = pos / (1e4 ** (dim // dim_model))\n","\n","    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"],"metadata":{"id":"0JcsePwCIpDZ","executionInfo":{"status":"ok","timestamp":1695474198101,"user_tz":-330,"elapsed":472,"user":{"displayName":"Nayan JHA","userId":"08243209700018571801"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["**We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.**\n","\n","Why should sinusoidal encodings extrapolate to longer sequence lengths? Because sine/cosine functions are periodic, and they cover a range of [0, 1]. Most other choices of encoding would not be periodic or restricted to the range [0, 1]. Suppose that, during inference, you provide an input sequence longer than any used during training. Positional encoding for the last elements in the sequence could be different than anything the model has seen before. For those reasons, and despite the fact that learned embeddings appeared to perform equally as well, the authors still chose to use sinusoidal encoding."],"metadata":{"id":"jmwKoe8JI0Hm"}},{"cell_type":"markdown","source":["### **The Transformer**\n","Finally, we’re ready to build the Transformer! Let’s take a look at the complete network diagram:\n","\n","<img src=\"https://miro.medium.com/max/1156/1*j9MmpNZzbBqkWes0GN8IBQ.png\">\n","\n","\n","Notice that the transformer uses an **encoder-decoder architecture**. The encoder (left) processes the input sequence and returns a feature vector (or memory vector). The decoder processes the target sequence, and incorporates information from the encoder memory. The output from the decoder is our model’s prediction!\n","\n","We can code the encoder/decoder modules independently of one another, and then combine them at the end. But first we need a few more pieces of information, which aren’t included in the figure above. For example, how should we choose to build the feed forward networks?"],"metadata":{"id":"AeLxTPbuI2-m"}},{"cell_type":"code","source":["def feed_forward(dim_input: int = 512, dim_feedforward: int = 2048) -> nn.Module:\n","    return nn.Sequential(\n","        nn.Linear(dim_input, dim_feedforward),\n","        nn.ReLU(),\n","        nn.Linear(dim_feedforward, dim_input),\n","    )"],"metadata":{"id":"s9B4Yc0ZIyBj","executionInfo":{"status":"ok","timestamp":1695474237452,"user_tz":-330,"elapsed":4,"user":{"displayName":"Nayan JHA","userId":"08243209700018571801"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["What kind of normalization should be used? Do we need any regularization, such as dropout layers?\n","\n","```\n","The output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. … We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.\n","```"],"metadata":{"id":"dBatVfTjI-RI"}},{"cell_type":"code","source":["class Residual(nn.Module):\n","    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1):\n","        super().__init__()\n","        self.sublayer = sublayer\n","        self.norm = nn.LayerNorm(dimension)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, *tensors: Tensor) -> Tensor:\n","        # Assume that the \"query\" tensor is given first, so we can compute the\n","        # residual.  This matches the signature of 'MultiHeadAttention'.\n","        return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))"],"metadata":{"id":"reS8Sa6-I7n0","executionInfo":{"status":"ok","timestamp":1695474259347,"user_tz":-330,"elapsed":9,"user":{"displayName":"Nayan JHA","userId":"08243209700018571801"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### **Encoder Layer**"],"metadata":{"id":"usxRs1gaJDB_"}},{"cell_type":"code","source":["class TransformerEncoderLayer(nn.Module):\n","    def __init__(\n","        self,\n","        dim_model: int = 512,\n","        num_heads: int = 6,\n","        dim_feedforward: int = 2048,\n","        dropout: float = 0.1,\n","    ):\n","        super().__init__()\n","        dim_q = dim_k = max(dim_model // num_heads, 1)\n","        self.attention = Residual(\n","            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n","            dimension=dim_model,\n","            dropout=dropout,\n","        )\n","        self.feed_forward = Residual(\n","            feed_forward(dim_model, dim_feedforward),\n","            dimension=dim_model,\n","            dropout=dropout,\n","        )\n","\n","    def forward(self, src: Tensor) -> Tensor:\n","        src = self.attention(src, src, src)\n","        return self.feed_forward(src)\n","\n","\n","class TransformerEncoder(nn.Module):\n","    def __init__(\n","        self,\n","        num_layers: int = 6,\n","        dim_model: int = 512,\n","        num_heads: int = 8,\n","        dim_feedforward: int = 2048,\n","        dropout: float = 0.1,\n","    ):\n","        super().__init__()\n","        self.layers = nn.ModuleList(\n","            [\n","                TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n","                for _ in range(num_layers)\n","            ]\n","        )\n","\n","    def forward(self, src: Tensor) -> Tensor:\n","        seq_len, dimension = src.size(1), src.size(2)\n","        src += position_encoding(seq_len, dimension)\n","        for layer in self.layers:\n","            src = layer(src)\n","\n","        return src\n"],"metadata":{"id":"ltxU7nfkJA-d","executionInfo":{"status":"ok","timestamp":1695474279171,"user_tz":-330,"elapsed":7,"user":{"displayName":"Nayan JHA","userId":"08243209700018571801"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["The decoder module is extremely similar. Just a few small differences:\n","\n","* The decoder accepts two arguments (target and memory), rather than one.\n","* There are two multi-head attention modules per layer, instead of one.\n","* The second multi-head attention accepts memory for two of its inputs."],"metadata":{"id":"47CoIJ0xJNpd"}},{"cell_type":"code","source":["class TransformerDecoderLayer(nn.Module):\n","    def __init__(\n","        self,\n","        dim_model: int = 512,\n","        num_heads: int = 6,\n","        dim_feedforward: int = 2048,\n","        dropout: float = 0.1,\n","    ):\n","        super().__init__()\n","        dim_q = dim_k = max(dim_model // num_heads, 1)\n","        self.attention_1 = Residual(\n","            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n","            dimension=dim_model,\n","            dropout=dropout,\n","        )\n","        self.attention_2 = Residual(\n","            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n","            dimension=dim_model,\n","            dropout=dropout,\n","        )\n","        self.feed_forward = Residual(\n","            feed_forward(dim_model, dim_feedforward),\n","            dimension=dim_model,\n","            dropout=dropout,\n","        )\n","\n","    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n","        tgt = self.attention_1(tgt, tgt, tgt)\n","        tgt = self.attention_2(tgt, memory, memory)\n","        return self.feed_forward(tgt)\n","\n","\n","class TransformerDecoder(nn.Module):\n","    def __init__(\n","        self,\n","        num_layers: int = 6,\n","        dim_model: int = 512,\n","        num_heads: int = 8,\n","        dim_feedforward: int = 2048,\n","        dropout: float = 0.1,\n","    ):\n","        super().__init__()\n","        self.layers = nn.ModuleList(\n","            [\n","                TransformerDecoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n","                for _ in range(num_layers)\n","            ]\n","        )\n","        self.linear = nn.Linear(dim_model, dim_model)\n","\n","    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n","        seq_len, dimension = tgt.size(1), tgt.size(2)\n","        tgt += position_encoding(seq_len, dimension)\n","        for layer in self.layers:\n","            tgt = layer(tgt, memory)\n","\n","        return torch.softmax(self.linear(tgt), dim=-1)"],"metadata":{"id":"VkcBFm_hJF1-","executionInfo":{"status":"ok","timestamp":1695474321184,"user_tz":-330,"elapsed":8,"user":{"displayName":"Nayan JHA","userId":"08243209700018571801"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Lastly, we need to wrap everything up into a single Transformer class. This requires minimal work, because it’s nothing new — just throw an encoder and decoder together, and pass data through them in the correct order."],"metadata":{"id":"x2Q6nJXVJS1A"}},{"cell_type":"code","source":["class Transformer(nn.Module):\n","    def __init__(\n","        self,\n","        num_encoder_layers: int = 6,\n","        num_decoder_layers: int = 6,\n","        dim_model: int = 512,\n","        num_heads: int = 6,\n","        dim_feedforward: int = 2048,\n","        dropout: float = 0.1,\n","        activation: nn.Module = nn.ReLU(),\n","    ):\n","        super().__init__()\n","        self.encoder = TransformerEncoder(\n","            num_layers=num_encoder_layers,\n","            dim_model=dim_model,\n","            num_heads=num_heads,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout,\n","        )\n","        self.decoder = TransformerDecoder(\n","            num_layers=num_decoder_layers,\n","            dim_model=dim_model,\n","            num_heads=num_heads,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout,\n","        )\n","\n","    def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n","        return self.decoder(tgt, self.encoder(src))"],"metadata":{"id":"W3j560Y4JP3P","executionInfo":{"status":"ok","timestamp":1695474341634,"user_tz":-330,"elapsed":9,"user":{"displayName":"Nayan JHA","userId":"08243209700018571801"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["And we’re done! Let’s create a simple test, as a sanity check for our implementation. We can construct random tensors for src and tgt, check that our model executes without errors, and confirm that the output tensor has the correct shape."],"metadata":{"id":"O4cilQAnJXaO"}},{"cell_type":"code","source":["src = torch.rand(64, 32, 512)\n","tgt = torch.rand(64, 16, 512)\n","out = Transformer()(src, tgt)\n","print(out.shape)\n","# torch.Size([64, 16, 512])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1IIkyTXZJVFs","executionInfo":{"status":"ok","timestamp":1695474365436,"user_tz":-330,"elapsed":5159,"user":{"displayName":"Nayan JHA","userId":"08243209700018571801"}},"outputId":"7478987a-14b7-412c-c4a6-347f2e4a3f38"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 16, 512])\n"]}]},{"cell_type":"markdown","source":["### **Conclusions**\n","I hope this helps to shed some light on Transformers, how they’re built, and how they work. Computer vision folks (like myself) may not have encountered these models before, but I expect to see much more of them in the next couple of years. DETR and ViT have already shown ground-breaking results. It’s only a matter of time before other SOTA models fall to Transformers as well. In particular, I’ll be waiting expectantly on end-to-end attention-based models for object detection, image segmentation, and image generation."],"metadata":{"id":"l7hzR6kPJeIs"}},{"cell_type":"code","source":[],"metadata":{"id":"iP7GanCLJekQ"},"execution_count":null,"outputs":[]}]}